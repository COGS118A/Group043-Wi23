{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Movies: Gross Income Prediction for Box Office Earnings\n",
    "\n",
    "# Names\n",
    "\n",
    "\n",
    "- Carly Freedman\n",
    "- Jackson Teel\n",
    "- Ye Yint Win\n",
    "- Garrett Dungca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "We would like to create a program to predict the gross income in U.S. dollars of movies based on their year, certificate, runtime, genre, rating and total number of votes. The certificate gives the movie rating (i.e. PG-13), the rating represents the total score received by the movies from reviewers on IMDB, and the total number of votes is how many people reviewed the movie total. We will be using a dataset containing all of these variables for every movie on IMDB. We are going to one-hot encode each of the categories. In order to predict the gross income we will be using and comparing several regression models such as decision tree regression, LASSO, Ridge, and linear regression. We will use a loss value based on the actual gross income vs. predicted gross income.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "There have been many studies conducted on predicting gross income of movies. However, there are few accurate ones. Most studies produce an accuracy in the 50-60% range. For example, a study conducted on this topic using both standard linear regression and classification via logistic regression only produced an accuracy of about 75% <a name=\"yoo\"></a>[<sup>[1]</sup>](#yoonote). Given that this study was likely limited by the types of analysis they performed, we would like to determine if there is any way to increase this accuracy using a wider variety of algorithms and a different dataset. In our own analysis, we will likely use standard linear regression as well, but as a baseline reference for our other algorithms.\n",
    "\n",
    "Using machine learning algorithms to predict gross income of movies has several benefits. Accurate income predictions can help movie studios, producers, and investors make better business decisions. It allows for more informed decisions to be made with regard to budgeting, marketing, and distribution<a name=\"dhir\"></a>[<sup>[2]</sup>](#dhirnote). These predictions can also help filmmakers and studios create movies that are more engaging to their audience. ML algorithms can be used to analyze audience preferences based on demographics and other key variables that help inform creative decisions. From the article titled, ‘Popularity prediction of movies: from statistical modeling to machine learning techniques’, we were inspired to use a Decision Tree model for our predictions as it turned out to be one of the more accurate models in their case. <a name=\"abidi\"></a>[<sup>[3]</sup>](#abidinote)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our problem is to predict the gross box office earnings of a movie based on features such as genre, rating, runtime, certificate, year, and votes. Logically, we would think that in some way metrics such as these which would describe the popularity of a movie would be a good indicator of how much revenue that movie will bring in. \n",
    "We are aiming to get a predicted value as close to the actual value as possible. We will be one-hot-encoding the only non-numerical value, genre, so we should be able to get good, interpretable results through metrics such as mean-squared-error (MSE), root-mean-squared-error (RMSE), mean average error (MAE), and R^2 Value. \n",
    "Given that our model provides reliable information when predicting gross box office earnings, the hope is for our model to help movie studios, producers, and investors to make more informed business decisions about budgeting, marketing, and distribution of their movies. Alternatively, our analysis may reveal that the features we used are not informative enough, even given a variety of complex models and approaches, in which we would determine that the data obtained from IMDb should not be used alone in future research and may need to include more complex features or deeper additional analysis such as sentiment analysis of reviews. Additionally, if we determine that our models’ accuracy stagnate even as we add more complexity (accounting for methods to prevent overfitting), we may conclude that the features we did use poorly map onto box office earnings, suggesting that IMDb data is wholly uninformative when determining the financial success of a movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Link to dataset: <a>https://www.kaggle.com/datasets/rajugc/imdb-movies-dataset-based-on-genre?resource=download</a>\n",
    "\n",
    "Column Descriptions:\n",
    "\n",
    " • movie_id - IMDB Movie ID\n",
    "\n",
    " • movie_name - Name of the movie\n",
    "\n",
    " • year - Release year\n",
    "\n",
    " • certificate - Certificate of the movie\n",
    "\n",
    " • run_time - Total movie run time\n",
    "\n",
    " • genre - Genre of the movie\n",
    "\n",
    " • rating - Rating of the movie\n",
    "\n",
    " • description - Description of the movie\n",
    "\n",
    " • director - Director of the movie\n",
    "\n",
    " • director_id - IMDB id of the director\n",
    "\n",
    " • star - Star of the movie\n",
    "\n",
    " • star_id - IMDB id of the star\n",
    "\n",
    " • votes - Number of votes in IMDB website\n",
    " \n",
    " • gross(in $) - Gross Box Office of the movie\n",
    "\n",
    "\n",
    "The dataset starts with 298975 total observations, and 14 variables. Once we remove the observations with Null values, we have 18709 total observations. Additionally, we are only interested in 6 of the variables as aforementioned because several of the variables are string values that would be too lengthy to one-hot encode. A valid observation must include the features: year (integer), certificate, runtime (float), genre, rating (float), votes (integer), and gross income (float), and it must also have no null values. These variables are a combination of ordinal variables and categorical variables which are able to be one-hot encoded. Therefore they will be usable by our program.\n",
    "\n",
    "Many of the movies in the dataset are listed under several genres. In total there are 40 unique genres listed in the dataset, but the data is organized into 13 folders: one for each main genre of movie. To save some computational expense in feature selection, we are going to only use the genre that corresponds to the directory that the movie was initially listed under. \n",
    "Additionally, in our data cleaning portion, we omit movies that are older than 1997. This is because the movie industry has undergone major changes the past couple decades and we want our machine learning model to be trained on data representative of the current movie market. \n",
    "\n",
    "The variables that we one-hot encode are genre and certificate. There are 27 unique certificates to start off with, but shorten this down to 6 since a handful of the certificates are old certificates that no longer exist or are show certificates. We also one-hot encode 12 unique genres in our dataset. These genres include (action, adventure, crime, family, fantasy, history, horror, mystery, scifi, sports, thriller, and war). Since we are not looking movies older than 1997, we decided to one-hot-encode the years in 5 year increments starting from 1995 to 2024 (e.g. 1995-2004, 2005-2009, …, 2019-2024) so that we can train them as an additional categorical feature.\n",
    "\n",
    "Last part of our data cleaning process is getting rid of outliers. We already eliminate a significant portion of outliers with regards to year when we only take the past 25 years into consideration but we also eliminate some of the outliers when it comes to runtime. Movies longer than 300 minutes will be omitted as our data contains two very extreme outliers (well over 1.5 times IQR) with regards to runtime that could potentially negatively impact the accuracy of our model.\n",
    "We check for multicollinearity using a correlation matrix. We will eliminate values with correlation coefficients greater than 0.7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>year</th>\n",
       "      <th>certificate</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>director</th>\n",
       "      <th>director_id</th>\n",
       "      <th>star</th>\n",
       "      <th>star_id</th>\n",
       "      <th>votes</th>\n",
       "      <th>gross(in $)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt5433140</td>\n",
       "      <td>Fast X</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>archive/crime.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dom Toretto and his family are targeted by the...</td>\n",
       "      <td>Louis Leterrier</td>\n",
       "      <td>/name/nm0504642/</td>\n",
       "      <td>Vin Diesel, \\nJordana Brewster, \\nTyrese Gibso...</td>\n",
       "      <td>/name/nm0004874/,/name/nm0108287/,/name/nm0879...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt11564570</td>\n",
       "      <td>Glass Onion</td>\n",
       "      <td>2022</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>139 min</td>\n",
       "      <td>archive/crime.csv</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Famed Southern detective Benoit Blanc travels ...</td>\n",
       "      <td>Rian Johnson</td>\n",
       "      <td>/name/nm0426059/</td>\n",
       "      <td>Daniel Craig, \\nEdward Norton, \\nKate Hudson, ...</td>\n",
       "      <td>/name/nm0185819/,/name/nm0001570/,/name/nm0005...</td>\n",
       "      <td>333315.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt10365998</td>\n",
       "      <td>Infinity Pool</td>\n",
       "      <td>2023</td>\n",
       "      <td>R</td>\n",
       "      <td>117 min</td>\n",
       "      <td>archive/crime.csv</td>\n",
       "      <td>6.5</td>\n",
       "      <td>James and Em Foster are enjoying an all-inclus...</td>\n",
       "      <td>Brandon Cronenberg</td>\n",
       "      <td>/name/nm0188722/</td>\n",
       "      <td>Alexander Skarsgård, \\nMia Goth, \\nCleopatra C...</td>\n",
       "      <td>/name/nm0002907/,/name/nm5301405/,/name/nm1671...</td>\n",
       "      <td>6955.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt14138650</td>\n",
       "      <td>The Pale Blue Eye</td>\n",
       "      <td>2022</td>\n",
       "      <td>R</td>\n",
       "      <td>128 min</td>\n",
       "      <td>archive/crime.csv</td>\n",
       "      <td>6.6</td>\n",
       "      <td>A world-weary detective is hired to investigat...</td>\n",
       "      <td>Scott Cooper</td>\n",
       "      <td>/name/nm0178376/</td>\n",
       "      <td>Christian Bale, \\nHarry Melling, \\nSimon McBur...</td>\n",
       "      <td>/name/nm0000288/,/name/nm0577982/,/name/nm0564...</td>\n",
       "      <td>85087.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt15789492</td>\n",
       "      <td>Infiesto</td>\n",
       "      <td>2023</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>96 min</td>\n",
       "      <td>archive/crime.csv</td>\n",
       "      <td>5.5</td>\n",
       "      <td>Two detectives are called to a small mining to...</td>\n",
       "      <td>Patxi Amezcua</td>\n",
       "      <td>/name/nm0025538/</td>\n",
       "      <td>Isak Férriz, \\nIria del Río, \\nAntonio Buíl, \\...</td>\n",
       "      <td>/name/nm1929945/,/name/nm4170579/,/name/nm0125...</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     movie_id         movie_name  year certificate  runtime  \\\n",
       "0   tt5433140             Fast X  2023         NaN      NaN   \n",
       "1  tt11564570        Glass Onion  2022       PG-13  139 min   \n",
       "2  tt10365998      Infinity Pool  2023           R  117 min   \n",
       "3  tt14138650  The Pale Blue Eye  2022           R  128 min   \n",
       "4  tt15789492           Infiesto  2023       TV-MA   96 min   \n",
       "\n",
       "               genre  rating  \\\n",
       "0  archive/crime.csv     NaN   \n",
       "1  archive/crime.csv     7.2   \n",
       "2  archive/crime.csv     6.5   \n",
       "3  archive/crime.csv     6.6   \n",
       "4  archive/crime.csv     5.5   \n",
       "\n",
       "                                         description            director  \\\n",
       "0  Dom Toretto and his family are targeted by the...     Louis Leterrier   \n",
       "1  Famed Southern detective Benoit Blanc travels ...        Rian Johnson   \n",
       "2  James and Em Foster are enjoying an all-inclus...  Brandon Cronenberg   \n",
       "3  A world-weary detective is hired to investigat...        Scott Cooper   \n",
       "4  Two detectives are called to a small mining to...       Patxi Amezcua   \n",
       "\n",
       "        director_id                                               star  \\\n",
       "0  /name/nm0504642/  Vin Diesel, \\nJordana Brewster, \\nTyrese Gibso...   \n",
       "1  /name/nm0426059/  Daniel Craig, \\nEdward Norton, \\nKate Hudson, ...   \n",
       "2  /name/nm0188722/  Alexander Skarsgård, \\nMia Goth, \\nCleopatra C...   \n",
       "3  /name/nm0178376/  Christian Bale, \\nHarry Melling, \\nSimon McBur...   \n",
       "4  /name/nm0025538/  Isak Férriz, \\nIria del Río, \\nAntonio Buíl, \\...   \n",
       "\n",
       "                                             star_id     votes  gross(in $)  \n",
       "0  /name/nm0004874/,/name/nm0108287/,/name/nm0879...       NaN          NaN  \n",
       "1  /name/nm0185819/,/name/nm0001570/,/name/nm0005...  333315.0          NaN  \n",
       "2  /name/nm0002907/,/name/nm5301405/,/name/nm1671...    6955.0          NaN  \n",
       "3  /name/nm0000288/,/name/nm0577982/,/name/nm0564...   85087.0          NaN  \n",
       "4  /name/nm1929945/,/name/nm4170579/,/name/nm0125...    2081.0          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'archive'               # use your path\n",
    "files = glob.glob(os.path.join(path, \"*.csv\"))  \n",
    "\n",
    "data_frames = []\n",
    "\n",
    "for file in files:\n",
    "    frame = pd.read_csv(file)\n",
    "    frame['genre'] = frame['genre'].apply(lambda x: str(file))\n",
    "    data_frames.append(frame)\n",
    "\n",
    "    \n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80576"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['year', 'certificate', 'runtime', 'rating']).reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1,440'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_497/2779369146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_runtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1157\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_497/2779369146.py\u001b[0m in \u001b[0;36mconvert_runtime\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Format runtimes into floats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runtime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_runtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '1,440'"
     ]
    }
   ],
   "source": [
    "#Format runtimes into floats\n",
    "def convert_runtime(string):\n",
    "    return float(string.split(' ')[0])\n",
    "    \n",
    "df['runtime'] = df['runtime'].apply(convert_runtime)\n",
    "df = df[df['runtime'] < 300]\n",
    "\n",
    "\n",
    "#Format genre by using only first value\n",
    "def genre_parse(genre):\n",
    "    print(genre)\n",
    "    genre = genre.split('/')[-1] # remove directory path\n",
    "    genre = genre[:-4]\n",
    "    \n",
    "    genre = genre.lower()\n",
    "    #print(genre)\n",
    "    return genre\n",
    "  \n",
    "df['genre']=df['genre'].apply(genre_parse)\n",
    "df = df.drop(df.loc[(df['genre'] == 'musical') | (df['genre'] == 'romance') | (df['genre'] == 'biography') | (df['genre']=='animation')].index)\n",
    "\n",
    "# Format Certificate filter out all data points to only include movies with the G, PG, PG-13, R, and NC-17 ratings, switch Not Rated to NR\n",
    "def convert_unrated(certificate):\n",
    "    if certificate == 'Not Rated':\n",
    "        return 'NR'\n",
    "    else:\n",
    "        return certificate\n",
    "    \n",
    "df=df.loc[(df['certificate'] == 'R') | (df['certificate'] =='PG-13') | (df['certificate'] == 'PG') | (df['certificate'] == 'Not Rated') | (df['certificate'] =='X') | (df['certificate'] == 'NC-17')]\n",
    "df['certificate']=df['certificate'].apply(convert_unrated)\n",
    "\n",
    "#Format years into ints and include only the past 25 years worth of data. \n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df.loc[(df['year'] >= 1997)].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Repeats and One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_eng = df\n",
    "genres = df['genre'].unique()\n",
    "for genre in genres:\n",
    "    df[genre] = 0\n",
    "names = df['movie_name'].unique()\n",
    "\n",
    "for name in names:\n",
    "    repeats = df.loc[(df['movie_name'] == name)]\n",
    "    genres = []\n",
    "    \n",
    "    for index in list(repeats.index):\n",
    "        genres.append(repeats.loc[index,'genre'])\n",
    "    \n",
    "    for genre in genres:\n",
    "        df.loc[list(repeats.index)[0], genre] = 1\n",
    "    \n",
    "    df = df.drop(list(repeats.index)[1:])\n",
    "    \n",
    "df=df.drop(['movie_name', 'genre'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale down gross earnings from dollars to millions of dollars\n",
    "df['gross(in $)'] = df['gross(in $)']/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.rc('font', size=50)          # controls default text sizes    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=50)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=50) \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 3, gridspec_kw={'wspace':1/8, 'hspace':1/8})\n",
    "\n",
    "df['year'].hist(bins=range(1997, 2022, 4), ax=ax1[0])\n",
    "ax1[0].set_xlabel('Year')\n",
    "ax1[0].set_ylabel('Count')\n",
    "ax1[0].set_title('Distribution of Year')\n",
    "\n",
    "df['certificate'].hist(ax=ax1[1])\n",
    "ax1[1].set_title('Distribution of Certificate')\n",
    "ax1[1].set_xlabel('Certificate')\n",
    "ax1[1].set_ylabel('Count')\n",
    "\n",
    "df['runtime'].hist(bins=range(0, 300, 30), ax=ax1[2])\n",
    "ax1[2].set_xlabel('Minutes')\n",
    "ax1[2].set_ylabel('Count')\n",
    "ax1[2].set_title('Distribution of Runtime')\n",
    "\n",
    "df_pre_eng['genre'].hist(ax=ax2[0],figsize=(80,50))\n",
    "ax2[0].set_title('Distribution of Genre')\n",
    "ax2[0].set_xlabel('Genre')\n",
    "ax2[0].set_ylabel('Count')\n",
    "\n",
    "df['rating'].hist(ax=ax2[1])\n",
    "ax2[1].set_title('Distribution of Rating')\n",
    "ax2[1].set_xlabel('Rating')\n",
    "ax2[1].set_ylabel('Count')\n",
    "\n",
    "df['votes'].hist(ax=ax2[2])\n",
    "ax2[2].set_title('Distribution of Number of Votes')\n",
    "ax2[2].set_xlabel('Number of Votes')\n",
    "ax2[2].set_ylabel('Count')\n",
    "\n",
    "df['gross(in $)'].hist(ax=ax3[0])\n",
    "ax3[0].set_title('Distribution of Gross Income (Millions of $)')\n",
    "ax3[0].set_xlabel('Gross Income (Millions of $)')\n",
    "ax3[0].set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Earliest Year:', df['year'].min())\n",
    "print('Latest Year:', df['year'].max())\n",
    "\n",
    "print(df_pre_eng['genre'].unique())\n",
    "\n",
    "print('Shortest Movie Runtime:', df['runtime'].min(), 'min')\n",
    "print('Longest Movie Runtime:', df['runtime'].max(), 'min')\n",
    "print('Average Movie Runtime:', df['runtime'].mean(), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(60,40))\n",
    "plt.rc('font', size=40)          # controls default text sizes    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=40)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=40) \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 2, gridspec_kw={'wspace':1/16, 'hspace':1/16}, figsize=(80, 80))\n",
    "\n",
    "sns.boxplot(df['year'],ax=ax1[0])\n",
    "ax1[0].set_title('Years of Release')\n",
    "ax1[0].set_ylabel('Year')\n",
    "\n",
    "sns.boxplot(df['runtime'],ax=ax1[1])\n",
    "ax1[1].set_title('Movie Runtimes')\n",
    "ax1[1].set_ylabel('Runtime')\n",
    "\n",
    "sns.boxplot(df['rating'],ax=ax2[0])\n",
    "ax2[0].set_title('Movie Rating Out of 10')\n",
    "ax2[0].set_ylabel('Rating')\n",
    "\n",
    "sns.boxplot(df['votes'],ax=ax2[1])\n",
    "ax2[1].set_title('Number of Votes for Movie')\n",
    "ax2[1].set_ylabel('Number of Votes')\n",
    "\n",
    "sns.boxplot(df['gross(in $)'],ax=ax3[0])\n",
    "ax3[0].set_title('Gross Income (Millions of $)')\n",
    "ax3[0].set_ylabel('Gross Income (Millions of $)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(ls):\n",
    "    q3= ls.quantile(0.75)\n",
    "    q1= ls.quantile(0.25)\n",
    "    IQR=q3-q1\n",
    "    outliers_upper = ls.loc[ls > q3 + 1.5 * IQR]\n",
    "    outliers_lower = df['runtime'].loc[df['runtime'] < q1 - 1.5 * IQR]\n",
    "    num_outliers_runtime = len(outliers_lower)+len(outliers_upper)\n",
    "    return num_outliers_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of outliers detected in year data: \" + str(outliers(df['year'])))\n",
    "print(\"Number of outliers detected in runtime data: \" + str(outliers(df['runtime'])))\n",
    "print(\"Number of outliers detected in rating data: \" + str(outliers(df['rating'])))\n",
    "print(\"Number of outliers detected in votes data: \" + str(outliers(df['votes'])))\n",
    "print(\"Number of outliers detected in gross income data: \" + str(outliers(df['gross(in $)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Correlation Matrix, dropping Film-Noir since none are in our dataframe anymore from the last 25 years\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We are going to use a few algorithms to approach the solution to this problem, but they are subsets of linear and nonlinear regression. This can be achieved through the use of sklearn's linear_model.LinearRegression. In addition to this, we will utilize a decision tree and support vector regressor as preliminary models, also in sklearn. Our problem is clearly a regression as we aim to predict gross earnings from our features, instead of a classification problem in which something like logistic regression would be preferred. To test our solution, we will run cross-validation on our dataset to generate an estimate of our model's performance. A baseline solution is a linear regression model on a simple 80-20 train-test split of our data, along with a correlation matrix that may illuminate underlying linear relationships in the data.\n",
    "\n",
    "Because our problem has a very simple goal to achieve, we want to introduce nuance via complex approaches to analyzing the algorithms we use. We’ll use a few different standard model approaches including Gradient Boosting and Random Forest, as well as more complex approaches such as XGBoost and compare the results across each model. Again, because of the simplicity of our problem, we can also extend our analysis to using some hyperparameter optimization techniques and hope to establish good baselines, especially since we are most likely to use repeated k-fold cross-validation given that our dataset has over 18000 examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "To predict the box office gross income of movies based on their year, certificate, runtime, genre, rating and total number of votes, we will be potentially using the following metrics. \n",
    "\n",
    "Mean Absolute Error to measure the average absolute difference between the predicted box office gross and the actual box office gross across all the movies.\n",
    "\n",
    "MAE = (1 / n) * sum(abs(y_true - y_pred))\n",
    "\n",
    "Mean Squared Error and Root Mean Squared Error to measure the square root of the average squared difference between the predicted box office gross and the actual box office gross.\n",
    "\n",
    "MSE = (1 / n) * sum((y_true - y_pred)^2)\n",
    "\n",
    "RMSE = sqrt((1 / n) * sum((y_true - y_pred)^2))\n",
    "\n",
    "And R2 Score to measure how well the regression model fits the actual data\n",
    "\n",
    "R2 Score = 1 - (sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2))\n",
    "\n",
    "y_true = actual box office gross\n",
    "\n",
    "y_pred = predicted box office gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Model Selection based on EDA\n",
    "\n",
    "From our EDA, we observe the relative distributions of our features. Year is relatively uniform, our certificates are largely R-rated movies on IMDb, runtime is normal and centered around ~2 hour movies, action and thriller movies are the largest genres, ratings are normal and centered around 5-6, votes are skewed positively, and gross earnings are skewed positively. The related skew with votes and gross earnings gives us reason to believe that these features are somewhat correlated, and it is likely that a general linear relationship with slight nonlinearities exists within our data. As such, we believe that models that work strongly on linear data would be a reasonable approach to our regression task.\n",
    "\n",
    "\n",
    "### Feature Selection / Data Transformation\n",
    "\n",
    "We predicted beforehand that given the wide span of data we had (year-wise), that an appropriate choice of trimming our data was to locate a uniform section of our data that we could perform regression on. We believed that too many complexities existed if we go back too far in time; for example, the greater proportion of film-noir that fell extremely out of popularity as the movie industry grew, the issue of inflation affecting earnings, and the small distribution of theaters and underdevelopment of movie culture. As such, we chose to isolate a subset of the data that seemed to have a steady (uniform) distribution of movies. In order to do that, we put the movies into bins based on year and identified that subset to be between 1997 to 2022. Additionally, in order to use the genre of a movie as a potential feature, we chose to one-hot encode it as the genre doesn’t seem to be ordinal in nature (i.e. action movies are not greater or lesser than history movies).\n",
    "\n",
    "### Baseline Performance Analysis\n",
    "\n",
    "Our initial assumptions of the data are somewhat confirmed by this correlation heatmap. Noting that votes and gross earnings are somewhat correlated at 0.7, we attempt an initial linear regression analysis to observe its performance on our data. Unfortunately, this performed significantly worse than the dataframe’s internal correlation function, showing an existent but weaker correlation. Nevertheless, this did not deter our future efforts to identify the relationship between our features and gross earnings as somewhat linear. Thus we continue to use MSE and R2 as primary predictors as MSE punishes poor predictions and R2 is the correlation we hope to identify.\n",
    "\n",
    "\n",
    "### Model Comparisons\n",
    "\n",
    "In our project we run five different models and compute the error metrics MSE and RMSE as well as review their correlation with the R-squared coefficient. We find that the linear regression performs the worst as it has the lowest correlation and the highest MSE. The decision tree outperformed the linear regression model with a higher correlation and lower MSE. The models with the best performance are the XGBoost, Random Forest, and the Support Vector Regressor which all had an R-squared value above 0.7 and smaller MSEs, however, of these three, the Random Forest has the smallest MSE. We also tried running a Grid Search algorithm the k-fold cross validation, however this model didn’t perform as well as the Random Forest meaning it had a smaller correlation score and higher MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[f for f in features if f not in ['gross(in $)']]]\n",
    "                                                    , df['gross(in $)'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train['rating'], bins=20)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Rating')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(X_test['rating'], bins=20)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Rating')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scaler = StandardScaler()\n",
    "ohe_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[[f for f in features if f not in ['gross(in $)', 'certificate']]]\n",
    "X_train_cat = X_train[['certificate']]\n",
    "\n",
    "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
    "X_train_cat_encoded = ohe_encoder.fit_transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_train_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]),\n",
    "    pd.DataFrame(X_train_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test[[f for f in features if f not in ['gross(in $)', 'certificate']]]\n",
    "X_test_cat = X_test[['certificate']]\n",
    "\n",
    "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
    "X_test_cat_encoded = ohe_encoder.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]),\n",
    "    pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Linear Regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Gross Box Office Earnings\")\n",
    "plt.ylabel(\"Predicted Gross Box Office Earnings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Decision Tree</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\">Using Year as Categorical (5-year increment)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_category'] = (((df['year'] - 1910) // 5) * 5 + 1910) // 10 * 10\n",
    "df['year_category'] = df['year_category'].apply(lambda x: str(x) + '-' + str(x+4) if (x+3) < 2023 else str(x) + '-2022')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = df['year_category'].value_counts().sort_index()\n",
    "year_counts.plot.bar()\n",
    "plt.title('Distribution of Year Category')\n",
    "plt.xlabel('Year Category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append('year_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[[f for f in features if f not in ['year', 'gross(in $)']]]\n",
    "                                                    , df['gross(in $)'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train['rating'], bins=20)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Rating')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(X_test['rating'], bins=20)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Rating')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Genres\n",
    "# Extract the genre columns from the train and test sets\n",
    "genre_cols = ['act', 'adv', 'cri', 'fam', 'fan', 'his', 'hor', 'mys', 'sci', 'spo', 'thr', 'war']\n",
    "X_train_genre = X_train[genre_cols]\n",
    "X_test_genre = X_test[genre_cols]\n",
    "\n",
    "# Plot the histograms\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train_genre.values, bins=20, label=genre_cols)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Genre')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "ax[1].hist(X_test_genre.values, bins=20, label=genre_cols)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Genre')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]]\n",
    "X_train_cat = X_train[['certificate', 'year_category']]\n",
    "\n",
    "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
    "X_train_cat_encoded = ohe_encoder.fit_transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_train_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "    pd.DataFrame(X_train_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate','year_category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]]\n",
    "X_test_cat = X_test[['certificate', 'year_category',]]\n",
    "\n",
    "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
    "X_test_cat_encoded = ohe_encoder.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "    pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Gross Box Office Earnings\")\n",
    "plt.ylabel(\"Predicted Gross Box Office Earnings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred= model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "feature_names = X_train_preprocessed.columns\n",
    "plt.figure(figsize=(50, 10))\n",
    "plot_tree(model, filled=True, feature_names=feature_names)\n",
    "plt.show()\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf', C=100, gamma='auto')\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = rf.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization on Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 1000]\n",
    "grid['learning_rate'] = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 5, 7]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=grid, cv=10)\n",
    "# search over our floating values\n",
    "grid_result = grid_search.fit(X=df[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]], y=df['gross(in $)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMAL: 0.460124 with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "-1.908803 (3.119256) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.908782 (3.118924) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.908568 (3.118712) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.748135 (2.908756) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.747606 (2.907288) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.748217 (2.907749) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.562902 (2.666039) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.563336 (2.666657) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.566180 (2.667023) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "-0.031009 (0.640922) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "-0.038492 (0.641626) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "-0.052397 (0.649654) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.905014 (3.115358) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.905419 (3.115248) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.905277 (3.114987) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.730182 (2.891633) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.731025 (2.892006) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.731429 (2.890582) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.529933 (2.636104) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.530504 (2.635610) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.532874 (2.634360) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.091174 (0.563240) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.085429 (0.563924) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.069836 (0.562684) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.903728 (3.114629) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.903973 (3.114374) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.903647 (3.114254) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.724582 (2.889216) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.724739 (2.888161) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.724244 (2.886642) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.518624 (2.629487) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.520363 (2.629334) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.519621 (2.626683) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.119153 (0.558221) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.115268 (0.551832) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.100270 (0.535648) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.747563 (2.906170) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.748803 (2.908487) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.747859 (2.907156) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.099727 (2.055599) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.105002 (2.063354) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.106225 (2.060612) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.572994 (1.357851) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.574847 (1.355510) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.578991 (1.356249) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.412467 (0.136139) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.409790 (0.138082) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.396795 (0.157718) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.731364 (2.892303) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.731108 (2.891539) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.730804 (2.890185) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.030992 (1.999241) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.034915 (1.999195) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.040571 (1.999732) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.469820 (1.278702) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.472434 (1.276782) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.479878 (1.273815) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.460124 (0.135768) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.453631 (0.138007) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.444410 (0.142801) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.723341 (2.888983) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.724306 (2.887481) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.723931 (2.886057) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.012353 (1.991259) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.010469 (1.988236) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.014156 (1.981385) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.440792 (1.266610) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.442537 (1.263222) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.444722 (1.248190) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.459345 (0.138288) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.451154 (0.137297) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.429277 (0.127082) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.562126 (2.665641) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.563008 (2.664362) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.564560 (2.664656) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.568387 (1.353272) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.575784 (1.355771) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.576555 (1.352257) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.028490 (0.635359) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.035887 (0.638121) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.049653 (0.646205) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.418119 (0.136650) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.413139 (0.137848) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.406975 (0.162318) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.525265 (2.632815) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.529679 (2.633274) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.530798 (2.632261) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.463978 (1.274888) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.470490 (1.276479) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.477773 (1.270790) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.092736 (0.558645) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.089262 (0.561926) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.072641 (0.558338) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.448705 (0.130804) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.454709 (0.133785) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.439519 (0.142345) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.516044 (2.623667) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.518678 (2.626706) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.517913 (2.624335) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.437745 (1.263656) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.439308 (1.259443) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.441953 (1.245087) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.112998 (0.553198) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.118710 (0.545230) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.103690 (0.531444) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.443402 (0.136197) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.442031 (0.129646) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.420387 (0.130727) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.549305 (1.319546) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.554560 (1.330685) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.561888 (1.335755) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.359359 (0.184497) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.345956 (0.188203) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.338836 (0.183842) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.405546 (0.137952) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.408407 (0.144753) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.396958 (0.153047) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.373746 (0.163250) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.386409 (0.148214) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.373729 (0.162081) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.460305 (1.259224) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.454846 (1.254751) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.462102 (1.248957) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.432995 (0.165981) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.429570 (0.163408) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.410608 (0.167196) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.456762 (0.132855) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.444041 (0.133213) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.433070 (0.142340) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.365787 (0.159524) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.396563 (0.161736) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.368527 (0.177576) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.434952 (1.235584) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.422734 (1.237843) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.424646 (1.221281) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.433613 (0.164600) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.434108 (0.159325) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.409799 (0.147443) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.451008 (0.141226) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.443895 (0.146431) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.429667 (0.129775) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.368352 (0.173595) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.387612 (0.181521) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.353125 (0.187560) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.001864 (0.600995) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.013682 (0.596613) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.016754 (0.602568) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.396932 (0.152632) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.409385 (0.142424) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.398373 (0.155815) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.395229 (0.134968) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.390236 (0.137482) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.407374 (0.158092) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.341013 (0.167623) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.336146 (0.163852) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.325380 (0.184672) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.110921 (0.532549) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.116321 (0.517377) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.102272 (0.522668) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.415569 (0.123479) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.441945 (0.141912) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.426083 (0.137655) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.437255 (0.143914) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.440981 (0.147393) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.425569 (0.143877) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.303702 (0.185871) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.349250 (0.171634) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.328390 (0.220500) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.136660 (0.510536) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.130773 (0.521542) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.125991 (0.491324) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.424097 (0.150552) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.424123 (0.131187) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.420777 (0.127174) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.405819 (0.133978) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.430700 (0.131868) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.397253 (0.136124) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.324418 (0.198770) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.354576 (0.167942) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.341779 (0.184882) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print('OPTIMAL: %f with params: %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "test_means = grid_result.cv_results_['mean_test_score']\n",
    "test_stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for test_mean, test_std, param in zip(test_means, test_stds, params):\n",
    "    print(\"%f (%f) with params: %r\" % (test_mean, test_std, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "After comparing multiple models and hypertuning some of the hyperparameters, we came to the conclusion that the Random Forest did the best job at modeling our data. The Random Forest seemed to model our data well since it can handle nonlinear data well, in our case, parameters like number of votes on IMDB and movie genre. It also is good at handling outliers in the data which is important in our case as our data contains a lot of points that fall outside of the 1.5 times the IQR range. We believe it’s this combination of characteristics that led our Random Forest model to have the greatest r-squared correlation and smallest mean-squared error. However, a second model that performed almost just as well was our XGBoost model, which does a great job at estimating the feature importance of the features in our dataset. For example, in our data set, the number of votes on IMDB was way more important than the other features in our data, followed by movie runtime and movie rating. The rest of the features seemed to have much less impact. This shows us why a model such as XGBoost performs significantly better than our linear regression model or decision tree model since these models weigh the input features equally. While the predictions of some of our better models are relatively accurate, they still may be too inaccurate to really be usable. The stronger models have a MAE error of around 17 units, which in our case would be 17 million dollars. In the context of our problem, probably a higher level of accuracy is needed.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "When we began the problem, the dataset appeared to be full of useful information. We started off with nearly 300,000 data points. After parsing through and eliminating null or unusable data, we were left with 80,000 observations. The data was formatted into 13 CSV files: one for each genre of movie. This resulted in the same movies being printed out twice. Also, because the genre column of the data contained many genres including ones not provided as CSV’s, it was very tricky to tell what a movie’s true genre was. We decided to label each movie as the genre or genres whose csv files they were listed under. \n",
    "After removing the duplicates we were left with around 10,000 data points. This was sufficient to get a model to run, but as evidenced by our accuracies it wasn’t the strongest base for training a model. If we had more usable data then we might have been able to improve the accuracy of the model by a considerable amount. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "The IMDB dataset is downloaded from the Kaggle website with no personal or sensitive information included from human participants. All the variables in the dataset are available to the public on IMDB website in which we can eliminate the concern for data privacy and misuse of the data. The movie ratings and box office gross are largely influenced by the viewers and fans of particular franchises and movie stars which may be biased, however, since we are using a large dataset with a huge number of ratings, the bias could be neglected or will be evaluated and addressed if the concerns were to arise. Our final product will be available to students of current COGS 118A class, future prospective students and potentially to the public and we will carefully monitor and identify the unintended use of our project.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We ran six different models to try to predict the gross income of movies based on their year of release, certificate, rating, runtime, genre, and number of votes. Overall, none of the models we tried ultimately yielded any significant results. This might be an indication that we lacked a sufficient amount of data. While the dataset started out extremely large, as aforementioned we had to cut out a significant portion of it in order for it to be usable. Another possible source of error is that the features we selected and were provided in the data just might not have been good indicators of the gross income of a movie. For example, the dataset provided the main star/actor in each movie, but we cut this out as it was categorical and illogical to one-hot-encode and therefore would not have worked with the models we chose to run. However maybe this, or more information that was omitted from the dataset entirely would have been better indicators. Perhaps we needed more information that was never in the dataset. The model that yielded the best results was the random forest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"doshinote\"></a>1.[^](#doshi): Doshi, Lyric, et al. Predicting Movie Prices through Dynamic Social Network Analysis - Core. https://core.ac.uk/download/pdf/82778929.pdf. <br> \n",
    "\n",
    "<a name=\"dhirnote\"></a>2.[^](#dhir): Dhir R, Raj A (2018) Movie success prediction using machine learning algorithms and their comparison. In: 2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC), IEEE, pp 385–390. <br>\n",
    "\n",
    "<a name='abidinote'></a>3.[^](#abidi): Abidi, S.M.R., Xu, Y., Ni, J. et al. Popularity prediction of movies: from statistical modeling to machine learning techniques. Multimed Tools Appl 79, 35583–35617 (2020). https://doi.org/10.1007/s11042-019-08546-5. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
