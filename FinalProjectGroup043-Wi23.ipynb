{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Movies: Gross Income Prediction for Box Office Earnings\n",
    "\n",
    "# Names\n",
    "\n",
    "\n",
    "- Carly Freedman\n",
    "- Jackson Teel\n",
    "- Ye Yint Win\n",
    "- Garrett Dungca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "We would like to create a program to predict the gross income in U.S. dollars of movies based on their year, certificate, runtime, genre, rating and total number of votes. The certificate gives the movie rating (i.e. PG-13), the rating represents the total score received by the movies from reviewers on IMDB, and the total number of votes is how many people reviewed the movie total. We will be using a dataset containing all of these variables for every movie on IMDB. We are going to one-hot encode each of the categories. In order to predict the gross income we will be using and comparing several regression models such as decision tree regression, LASSO, Ridge, and linear regression. We will use a loss value based on the actual gross income vs. predicted gross income."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "There have been many studies conducted on predicting gross income of movies. However, there are few accurate ones. Most studies produce an accuracy in the 50-60% range. For example, a study conducted on this topic using both standard linear regression and classification via logistic regression only produced an accuracy of about 75% <a name=\"yoo\"></a>[<sup>[1]</sup>](#yoonote). Given that this study was likely limited by the types of analysis they performed, we would like to determine if there is any way to increase this accuracy using a wider variety of algorithms and a different dataset. In our own analysis, we will likely use standard linear regression as well, but as a baseline reference for our other algorithms.\n",
    "\n",
    "Using machine learning algorithms to predict gross income of movies has several benefits. Accurate income predictions can help movie studios, producers, and investors make better business decisions. It allows for more informed decisions to be made with regard to budgeting, marketing, and distribution<a name=\"dhir\"></a>[<sup>[2]</sup>](#dhirnote). These predictions can also help filmmakers and studios create movies that are more engaging to their audience. ML algorithms can be used to analyze audience preferences based on demographics and other key variables that help inform creative decisions. From the article titled, ‘Popularity prediction of movies: from statistical modeling to machine learning techniques’, we were inspired to use a Decision Tree model for our predictions as it turned out to be one of the more accurate models in their case. <a name=\"abidi\"></a>[<sup>[3]</sup>](#abidinote)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our problem is how to predict the gross box office earnings of a movie based on features such as genre, rating, runtime, certificate, year, and votes. The metric we aim to optimize is the loss of the amount in US dollars of gross box office earnings compared with the predicted values, using IMDB's labeled dataset of movies. Our specific loss is undetermined at the moment, it is more likely that we will explore a variety of losses given the wide range of earnings a movie can make. This includes but is not limited to:\n",
    "MAPE: Good for movies earning greater amounts even if the prediction is off, worse for predictions with low earning movies (proportion is greater with smaller numbers)\n",
    "MSE: Useful but difficult to interpret especially with heavy mispredictions (could choose to look at earnings in # of millions of dollars instead)\n",
    "MAE: Simple but doesn’t punish bad predictions as obviously as MSE\n",
    "RMSE: Addresses the issue of earnings being in a large number amount but more difficult to interpret\n",
    "\n",
    "Given that our model provides reliable information when predicting gross box office earnings, the hope is for our model to help movie studios, producers, and investors to make more informed business decisions about budgeting, marketing, and distribution of their movies. Alternatively, our analysis may reveal that the features we used are not informative enough, even given a variety of complex models and approaches, in which we would determine that the data obtained from IMDb should not be used alone in future research and may need to include more complex features or deeper additional analysis such as sentiment analysis of reviews. Additionally, if we determine that our models’ accuracy stagnate even as we add more complexity (accounting for methods to prevent overfitting), we may conclude that the features we did use poorly map onto box office earnings, suggesting that IMDb data is wholly uninformative when determining the financial success of a movie. As stated in the problem statement, we will likely explore multiple kinds of losses/error metrics, mostly because individually each metric can be both good and bad based on our data. In the first reference (“Predicting Movie Revenue from IMDb Data”), MAPE is particularly noted to be a poor metric because of the issue of lower earning movies having greater error on worse mispredictions [1]. Their data, however, only looks at 4052 movies whereas we have over 18000, meaning there is a possibility that their analysis may be not entirely informed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Link to dataset: <a>https://www.kaggle.com/datasets/rajugc/imdb-movies-dataset-based-on-genre?resource=download</a>\n",
    "\n",
    "Column Descriptions:\n",
    "\n",
    " • movie_id - IMDB Movie ID\n",
    "\n",
    " • movie_name - Name of the movie\n",
    "\n",
    " • year - Release year\n",
    "\n",
    " • certificate - Certificate of the movie\n",
    "\n",
    " • run_time - Total movie run time\n",
    "\n",
    " • genre - Genre of the movie\n",
    "\n",
    " • rating - Rating of the movie\n",
    "\n",
    " • description - Description of the movie\n",
    "\n",
    " • director - Director of the movie\n",
    "\n",
    " • director_id - IMDB id of the director\n",
    "\n",
    " • star - Star of the movie\n",
    "\n",
    " • star_id - IMDB id of the star\n",
    "\n",
    " • votes - Number of votes in IMDB website\n",
    " \n",
    " • gross(in $) - Gross Box Office of the movie\n",
    "\n",
    "\n",
    "The dataset starts with 298975 total observations, and 14 variables. Once we remove the observations with Null values, we have 18709 total observations. Additionally, we are only interested in 6 of the variables as aforementioned because several of the variables are string values that would be too lengthy to one-hot encode. A valid observation must include the features: year (integer), certificate, runtime (integer), genre, rating (float value),  votes (integer), and gross income (float), and it must also have no null values. These variables are a combination of ordinal variables and categorical variables which are able to be one-hot encoded. Therefore they will be usable by our program.\n",
    "\n",
    "Many of the movies in the dataset are listed under several genres. In total there are 40 unique genres listed in the dataset, but the data is organized into 13 folders: one for each main genre of movie. To save some computational expense in feature selection, we are going to only use the genre that corresponds to the directory that the movie was initially listed under. For the same reason, we will chunk the year data into groups of 15 years. \n",
    "\n",
    "The variables that we will one-hot encode will include genre and certificate. There are 27 unique certificates and 13 unique genres in our dataset that we are interested in (action, adventure, crime, family, fantasy, film-noir, history, horror, mystery, scifi, sports, thriller, and war). We can also one-hot encode year by first grouping movies into a new feature which will represent a range of years (e.g. 1975-1999 movies) and placing movies into that range if their year is within it, and one-hot encoding each entry of the list of ranges (e.g. 1950-1974, 1975-1999, 2000-2022).\n",
    "\n",
    "We are going to eliminate all outliers from the year, runtime, rating, votes, and gross income categories. In order to do so we will discard all dataset outside of 1.5*IQR above and below the upper and lower quartiles. \n",
    "\n",
    "We will check for multicollinearity using a correlation matrix. We will eliminate values with correlation coefficients greater than 0.7."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>year</th>\n",
       "      <th>certificate</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>director</th>\n",
       "      <th>director_id</th>\n",
       "      <th>star</th>\n",
       "      <th>star_id</th>\n",
       "      <th>votes</th>\n",
       "      <th>gross(in $)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt9114286</td>\n",
       "      <td>Black Panther: Wakanda Forever</td>\n",
       "      <td>2022</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>161 min</td>\n",
       "      <td>./data\\action.csv</td>\n",
       "      <td>6.9</td>\n",
       "      <td>The people of Wakanda fight to protect their h...</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>/name/nm3363032/</td>\n",
       "      <td>Letitia Wright, \\nLupita Nyong'o, \\nDanai Guri...</td>\n",
       "      <td>/name/nm4004793/,/name/nm2143282/,/name/nm1775...</td>\n",
       "      <td>204835.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1630029</td>\n",
       "      <td>Avatar: The Way of Water</td>\n",
       "      <td>2022</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>192 min</td>\n",
       "      <td>./data\\action.csv</td>\n",
       "      <td>7.8</td>\n",
       "      <td>Jake Sully lives with his newfound family form...</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>/name/nm0000116/</td>\n",
       "      <td>Sam Worthington, \\nZoe Saldana, \\nSigourney We...</td>\n",
       "      <td>/name/nm0941777/,/name/nm0757855/,/name/nm0000...</td>\n",
       "      <td>295119.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt5884796</td>\n",
       "      <td>Plane</td>\n",
       "      <td>2023</td>\n",
       "      <td>R</td>\n",
       "      <td>107 min</td>\n",
       "      <td>./data\\action.csv</td>\n",
       "      <td>6.5</td>\n",
       "      <td>A pilot finds himself caught in a war zone aft...</td>\n",
       "      <td>Jean-François Richet</td>\n",
       "      <td>/name/nm0724938/</td>\n",
       "      <td>Gerard Butler, \\nMike Colter, \\nTony Goldwyn, ...</td>\n",
       "      <td>/name/nm0124930/,/name/nm1591496/,/name/nm0001...</td>\n",
       "      <td>26220.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt6710474</td>\n",
       "      <td>Everything Everywhere All at Once</td>\n",
       "      <td>2022</td>\n",
       "      <td>R</td>\n",
       "      <td>139 min</td>\n",
       "      <td>./data\\action.csv</td>\n",
       "      <td>8.0</td>\n",
       "      <td>A middle-aged Chinese immigrant is swept up in...</td>\n",
       "      <td>Dan Kwan, \\nDaniel Scheinert</td>\n",
       "      <td>/name/nm3453283/</td>\n",
       "      <td>Michelle Yeoh, \\nStephanie Hsu, \\nJamie Lee Cu...</td>\n",
       "      <td>/name/nm3215397/,/name/nm0000706/,/name/nm3513...</td>\n",
       "      <td>327858.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt5433140</td>\n",
       "      <td>Fast X</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>./data\\action.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dom Toretto and his family are targeted by the...</td>\n",
       "      <td>Louis Leterrier</td>\n",
       "      <td>/name/nm0504642/</td>\n",
       "      <td>Vin Diesel, \\nJordana Brewster, \\nTyrese Gibso...</td>\n",
       "      <td>/name/nm0004874/,/name/nm0108287/,/name/nm0879...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_id                         movie_name  year certificate  runtime  \\\n",
       "0  tt9114286     Black Panther: Wakanda Forever  2022       PG-13  161 min   \n",
       "1  tt1630029           Avatar: The Way of Water  2022       PG-13  192 min   \n",
       "2  tt5884796                              Plane  2023           R  107 min   \n",
       "3  tt6710474  Everything Everywhere All at Once  2022           R  139 min   \n",
       "4  tt5433140                             Fast X  2023         NaN      NaN   \n",
       "\n",
       "               genre  rating  \\\n",
       "0  ./data\\action.csv     6.9   \n",
       "1  ./data\\action.csv     7.8   \n",
       "2  ./data\\action.csv     6.5   \n",
       "3  ./data\\action.csv     8.0   \n",
       "4  ./data\\action.csv     NaN   \n",
       "\n",
       "                                         description  \\\n",
       "0  The people of Wakanda fight to protect their h...   \n",
       "1  Jake Sully lives with his newfound family form...   \n",
       "2  A pilot finds himself caught in a war zone aft...   \n",
       "3  A middle-aged Chinese immigrant is swept up in...   \n",
       "4  Dom Toretto and his family are targeted by the...   \n",
       "\n",
       "                       director       director_id  \\\n",
       "0                  Ryan Coogler  /name/nm3363032/   \n",
       "1                 James Cameron  /name/nm0000116/   \n",
       "2          Jean-François Richet  /name/nm0724938/   \n",
       "3  Dan Kwan, \\nDaniel Scheinert  /name/nm3453283/   \n",
       "4               Louis Leterrier  /name/nm0504642/   \n",
       "\n",
       "                                                star  \\\n",
       "0  Letitia Wright, \\nLupita Nyong'o, \\nDanai Guri...   \n",
       "1  Sam Worthington, \\nZoe Saldana, \\nSigourney We...   \n",
       "2  Gerard Butler, \\nMike Colter, \\nTony Goldwyn, ...   \n",
       "3  Michelle Yeoh, \\nStephanie Hsu, \\nJamie Lee Cu...   \n",
       "4  Vin Diesel, \\nJordana Brewster, \\nTyrese Gibso...   \n",
       "\n",
       "                                             star_id     votes  gross(in $)  \n",
       "0  /name/nm4004793/,/name/nm2143282/,/name/nm1775...  204835.0          NaN  \n",
       "1  /name/nm0941777/,/name/nm0757855/,/name/nm0000...  295119.0          NaN  \n",
       "2  /name/nm0124930/,/name/nm1591496/,/name/nm0001...   26220.0          NaN  \n",
       "3  /name/nm3215397/,/name/nm0000706/,/name/nm3513...  327858.0          NaN  \n",
       "4  /name/nm0004874/,/name/nm0108287/,/name/nm0879...       NaN          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'./data'                     # use your path\n",
    "files = glob.glob(os.path.join(path, \"*.csv\"))  \n",
    "\n",
    "data_frames = []\n",
    "\n",
    "for file in files:\n",
    "    frame = pd.read_csv(file)\n",
    "    frame['genre'] = frame['genre'].apply(lambda x: str(file))\n",
    "    data_frames.append(frame)\n",
    "\n",
    "    \n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5038"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['year', 'certificate', 'runtime', 'rating']).reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>year</th>\n",
       "      <th>certificate</th>\n",
       "      <th>runtime</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>director</th>\n",
       "      <th>director_id</th>\n",
       "      <th>star</th>\n",
       "      <th>star_id</th>\n",
       "      <th>votes</th>\n",
       "      <th>gross(in $)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1825683</td>\n",
       "      <td>Black Panther</td>\n",
       "      <td>2018</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>134.0</td>\n",
       "      <td>act</td>\n",
       "      <td>7.3</td>\n",
       "      <td>T'Challa, heir to the hidden but advanced king...</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>/name/nm3363032/</td>\n",
       "      <td>Chadwick Boseman, \\nMichael B. Jordan, \\nLupit...</td>\n",
       "      <td>/name/nm1569276/,/name/nm0430107/,/name/nm2143...</td>\n",
       "      <td>785813.0</td>\n",
       "      <td>700059566.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0499549</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>2009</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>162.0</td>\n",
       "      <td>act</td>\n",
       "      <td>7.9</td>\n",
       "      <td>A paraplegic Marine dispatched to the moon Pan...</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>/name/nm0000116/</td>\n",
       "      <td>Sam Worthington, \\nZoe Saldana, \\nSigourney We...</td>\n",
       "      <td>/name/nm0941777/,/name/nm0757855/,/name/nm0000...</td>\n",
       "      <td>1322694.0</td>\n",
       "      <td>760507625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt1392170</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>2012</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>142.0</td>\n",
       "      <td>act</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Katniss Everdeen voluntarily takes her younger...</td>\n",
       "      <td>Gary Ross</td>\n",
       "      <td>/name/nm0002657/</td>\n",
       "      <td>Jennifer Lawrence, \\nJosh Hutcherson, \\nLiam H...</td>\n",
       "      <td>/name/nm2225369/,/name/nm1242688/,/name/nm2955...</td>\n",
       "      <td>927499.0</td>\n",
       "      <td>408010692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt1160419</td>\n",
       "      <td>Dune</td>\n",
       "      <td>2021</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>155.0</td>\n",
       "      <td>act</td>\n",
       "      <td>8.0</td>\n",
       "      <td>A noble family becomes embroiled in a war for ...</td>\n",
       "      <td>Denis Villeneuve</td>\n",
       "      <td>/name/nm0898288/</td>\n",
       "      <td>Timothée Chalamet, \\nRebecca Ferguson, \\nZenda...</td>\n",
       "      <td>/name/nm3154303/,/name/nm0272581/,/name/nm3918...</td>\n",
       "      <td>649342.0</td>\n",
       "      <td>108327830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0120737</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>178.0</td>\n",
       "      <td>act</td>\n",
       "      <td>8.8</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "      <td>Peter Jackson</td>\n",
       "      <td>/name/nm0001392/</td>\n",
       "      <td>Elijah Wood, \\nIan McKellen, \\nOrlando Bloom, ...</td>\n",
       "      <td>/name/nm0000704/,/name/nm0005212/,/name/nm0089...</td>\n",
       "      <td>1889727.0</td>\n",
       "      <td>315544750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_id                                         movie_name  year  \\\n",
       "0  tt1825683                                      Black Panther  2018   \n",
       "1  tt0499549                                             Avatar  2009   \n",
       "2  tt1392170                                   The Hunger Games  2012   \n",
       "3  tt1160419                                               Dune  2021   \n",
       "4  tt0120737  The Lord of the Rings: The Fellowship of the Ring  2001   \n",
       "\n",
       "  certificate  runtime genre  rating  \\\n",
       "0       PG-13    134.0   act     7.3   \n",
       "1       PG-13    162.0   act     7.9   \n",
       "2       PG-13    142.0   act     7.2   \n",
       "3       PG-13    155.0   act     8.0   \n",
       "4       PG-13    178.0   act     8.8   \n",
       "\n",
       "                                         description          director  \\\n",
       "0  T'Challa, heir to the hidden but advanced king...      Ryan Coogler   \n",
       "1  A paraplegic Marine dispatched to the moon Pan...     James Cameron   \n",
       "2  Katniss Everdeen voluntarily takes her younger...         Gary Ross   \n",
       "3  A noble family becomes embroiled in a war for ...  Denis Villeneuve   \n",
       "4  A meek Hobbit from the Shire and eight compani...     Peter Jackson   \n",
       "\n",
       "        director_id                                               star  \\\n",
       "0  /name/nm3363032/  Chadwick Boseman, \\nMichael B. Jordan, \\nLupit...   \n",
       "1  /name/nm0000116/  Sam Worthington, \\nZoe Saldana, \\nSigourney We...   \n",
       "2  /name/nm0002657/  Jennifer Lawrence, \\nJosh Hutcherson, \\nLiam H...   \n",
       "3  /name/nm0898288/  Timothée Chalamet, \\nRebecca Ferguson, \\nZenda...   \n",
       "4  /name/nm0001392/  Elijah Wood, \\nIan McKellen, \\nOrlando Bloom, ...   \n",
       "\n",
       "                                             star_id      votes  gross(in $)  \n",
       "0  /name/nm1569276/,/name/nm0430107/,/name/nm2143...   785813.0  700059566.0  \n",
       "1  /name/nm0941777/,/name/nm0757855/,/name/nm0000...  1322694.0  760507625.0  \n",
       "2  /name/nm2225369/,/name/nm1242688/,/name/nm2955...   927499.0  408010692.0  \n",
       "3  /name/nm3154303/,/name/nm0272581/,/name/nm3918...   649342.0  108327830.0  \n",
       "4  /name/nm0000704/,/name/nm0005212/,/name/nm0089...  1889727.0  315544750.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Format runtimes into floats\n",
    "def convert_runtime(string):\n",
    "    return float(string.split(' ')[0])\n",
    "    \n",
    "df['runtime'] = df['runtime'].apply(convert_runtime)\n",
    "df = df[df['runtime'] < 300]\n",
    "\n",
    "\n",
    "#Format genre by using only first value\n",
    "def genre_parse(genre):\n",
    "    genre = str(genre).split(\"\\\\\")\n",
    "    genre = genre[1].split('.')[0]\n",
    "    \n",
    "    genre = genre.lower()\n",
    "    return genre[:3]\n",
    "  \n",
    "df['genre']=df['genre'].apply(genre_parse)\n",
    "df = df.drop(df.loc[(df['genre'] == 'musical') | (df['genre'] == 'romance') | (df['genre'] == 'biography') | (df['genre']=='animation')].index)\n",
    "\n",
    "# Format Certificate filter out all data points to only include movies with the G, PG, PG-13, R, and NC-17 ratings, switch Not Rated to NR\n",
    "def convert_unrated(certificate):\n",
    "    if certificate == 'Not Rated':\n",
    "        return 'NR'\n",
    "    else:\n",
    "        return certificate\n",
    "    \n",
    "df=df.loc[(df['certificate'] == 'R') | (df['certificate'] =='PG-13') | (df['certificate'] == 'PG') | (df['certificate'] == 'Not Rated') | (df['certificate'] =='X') | (df['certificate'] == 'NC-17')]\n",
    "df['certificate']=df['certificate'].apply(convert_unrated)\n",
    "\n",
    "#Format years into ints and include only the past 25 years worth of data. \n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df.loc[(df['year'] >= 1997)].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Repeats and One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>year</th>\n",
       "      <th>certificate</th>\n",
       "      <th>runtime</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>director</th>\n",
       "      <th>director_id</th>\n",
       "      <th>star</th>\n",
       "      <th>star_id</th>\n",
       "      <th>...</th>\n",
       "      <th>cri</th>\n",
       "      <th>fam</th>\n",
       "      <th>fan</th>\n",
       "      <th>his</th>\n",
       "      <th>hor</th>\n",
       "      <th>mys</th>\n",
       "      <th>sci</th>\n",
       "      <th>spo</th>\n",
       "      <th>thr</th>\n",
       "      <th>war</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt1825683</td>\n",
       "      <td>2018</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>134.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>T'Challa, heir to the hidden but advanced king...</td>\n",
       "      <td>Ryan Coogler</td>\n",
       "      <td>/name/nm3363032/</td>\n",
       "      <td>Chadwick Boseman, \\nMichael B. Jordan, \\nLupit...</td>\n",
       "      <td>/name/nm1569276/,/name/nm0430107/,/name/nm2143...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0499549</td>\n",
       "      <td>2009</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>162.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>A paraplegic Marine dispatched to the moon Pan...</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>/name/nm0000116/</td>\n",
       "      <td>Sam Worthington, \\nZoe Saldana, \\nSigourney We...</td>\n",
       "      <td>/name/nm0941777/,/name/nm0757855/,/name/nm0000...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt1392170</td>\n",
       "      <td>2012</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>142.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Katniss Everdeen voluntarily takes her younger...</td>\n",
       "      <td>Gary Ross</td>\n",
       "      <td>/name/nm0002657/</td>\n",
       "      <td>Jennifer Lawrence, \\nJosh Hutcherson, \\nLiam H...</td>\n",
       "      <td>/name/nm2225369/,/name/nm1242688/,/name/nm2955...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt1160419</td>\n",
       "      <td>2021</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>155.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>A noble family becomes embroiled in a war for ...</td>\n",
       "      <td>Denis Villeneuve</td>\n",
       "      <td>/name/nm0898288/</td>\n",
       "      <td>Timothée Chalamet, \\nRebecca Ferguson, \\nZenda...</td>\n",
       "      <td>/name/nm3154303/,/name/nm0272581/,/name/nm3918...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0120737</td>\n",
       "      <td>2001</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>178.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "      <td>Peter Jackson</td>\n",
       "      <td>/name/nm0001392/</td>\n",
       "      <td>Elijah Wood, \\nIan McKellen, \\nOrlando Bloom, ...</td>\n",
       "      <td>/name/nm0000704/,/name/nm0005212/,/name/nm0089...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_id  year certificate  runtime  rating  \\\n",
       "0  tt1825683  2018       PG-13    134.0     7.3   \n",
       "1  tt0499549  2009       PG-13    162.0     7.9   \n",
       "2  tt1392170  2012       PG-13    142.0     7.2   \n",
       "3  tt1160419  2021       PG-13    155.0     8.0   \n",
       "4  tt0120737  2001       PG-13    178.0     8.8   \n",
       "\n",
       "                                         description          director  \\\n",
       "0  T'Challa, heir to the hidden but advanced king...      Ryan Coogler   \n",
       "1  A paraplegic Marine dispatched to the moon Pan...     James Cameron   \n",
       "2  Katniss Everdeen voluntarily takes her younger...         Gary Ross   \n",
       "3  A noble family becomes embroiled in a war for ...  Denis Villeneuve   \n",
       "4  A meek Hobbit from the Shire and eight compani...     Peter Jackson   \n",
       "\n",
       "        director_id                                               star  \\\n",
       "0  /name/nm3363032/  Chadwick Boseman, \\nMichael B. Jordan, \\nLupit...   \n",
       "1  /name/nm0000116/  Sam Worthington, \\nZoe Saldana, \\nSigourney We...   \n",
       "2  /name/nm0002657/  Jennifer Lawrence, \\nJosh Hutcherson, \\nLiam H...   \n",
       "3  /name/nm0898288/  Timothée Chalamet, \\nRebecca Ferguson, \\nZenda...   \n",
       "4  /name/nm0001392/  Elijah Wood, \\nIan McKellen, \\nOrlando Bloom, ...   \n",
       "\n",
       "                                             star_id  ...  cri  fam  fan  his  \\\n",
       "0  /name/nm1569276/,/name/nm0430107/,/name/nm2143...  ...    0    0    0    0   \n",
       "1  /name/nm0941777/,/name/nm0757855/,/name/nm0000...  ...    0    0    1    0   \n",
       "2  /name/nm2225369/,/name/nm1242688/,/name/nm2955...  ...    0    0    0    0   \n",
       "3  /name/nm3154303/,/name/nm0272581/,/name/nm3918...  ...    0    0    0    0   \n",
       "4  /name/nm0000704/,/name/nm0005212/,/name/nm0089...  ...    0    0    1    0   \n",
       "\n",
       "   hor  mys  sci  spo  thr  war  \n",
       "0    0    0    1    0    0    0  \n",
       "1    0    0    1    0    0    0  \n",
       "2    0    0    1    0    1    0  \n",
       "3    0    0    1    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre_eng = df\n",
    "genres = df['genre'].unique()\n",
    "for genre in genres:\n",
    "    df[genre] = 0\n",
    "names = df['movie_name'].unique()\n",
    "\n",
    "for name in names:\n",
    "    repeats = df.loc[(df['movie_name'] == name)]\n",
    "    genres = []\n",
    "    \n",
    "    for index in list(repeats.index):\n",
    "        genres.append(repeats.loc[index,'genre'])\n",
    "    \n",
    "    for genre in genres:\n",
    "        df.loc[list(repeats.index)[0], genre] = 1\n",
    "    \n",
    "    df = df.drop(list(repeats.index)[1:])\n",
    "    \n",
    "df=df.drop(['movie_name', 'genre'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale down gross earnings from dollars to millions of dollars\n",
    "df['gross(in $)'] = df['gross(in $)']/1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "plt.figure(figsize=(40,40))\n",
    "plt.rc('font', size=50)          # controls default text sizes    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=50)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=50) \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 3, gridspec_kw={'wspace':1/8, 'hspace':1/8})\n",
    "\n",
    "df['year'].hist(bins=range(1997, 2022, 4), ax=ax1[0])\n",
    "ax1[0].set_xlabel('Year')\n",
    "ax1[0].set_ylabel('Count')\n",
    "ax1[0].set_title('Distribution of Year')\n",
    "\n",
    "df['certificate'].hist(ax=ax1[1])\n",
    "ax1[1].set_title('Distribution of Certificate')\n",
    "ax1[1].set_xlabel('Certificate')\n",
    "ax1[1].set_ylabel('Count')\n",
    "\n",
    "df['runtime'].hist(bins=range(0, 300, 30), ax=ax1[2])\n",
    "ax1[2].set_xlabel('Minutes')\n",
    "ax1[2].set_ylabel('Count')\n",
    "ax1[2].set_title('Distribution of Runtime')\n",
    "\n",
    "df_pre_eng['genre'].hist(ax=ax2[0],figsize=(80,50))\n",
    "ax2[0].set_title('Distribution of Genre')\n",
    "ax2[0].set_xlabel('Genre')\n",
    "ax2[0].set_ylabel('Count')\n",
    "\n",
    "df['rating'].hist(ax=ax2[1])\n",
    "ax2[1].set_title('Distribution of Rating')\n",
    "ax2[1].set_xlabel('Rating')\n",
    "ax2[1].set_ylabel('Count')\n",
    "\n",
    "df['votes'].hist(ax=ax2[2])\n",
    "ax2[2].set_title('Distribution of Number of Votes')\n",
    "ax2[2].set_xlabel('Number of Votes')\n",
    "ax2[2].set_ylabel('Count')\n",
    "\n",
    "df['gross(in $)'].hist(ax=ax3[0])\n",
    "ax3[0].set_title('Distribution of Gross Income (Millions of $)')\n",
    "ax3[0].set_xlabel('Gross Income (Millions of $)')\n",
    "ax3[0].set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Earliest Year:', df['year'].min())\n",
    "print('Latest Year:', df['year'].max())\n",
    "\n",
    "print(df_pre_eng['genre'].unique())\n",
    "\n",
    "print('Shortest Movie Runtime:', df['runtime'].min(), 'min')\n",
    "print('Longest Movie Runtime:', df['runtime'].max(), 'min')\n",
    "print('Average Movie Runtime:', df['runtime'].mean(), 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(60,40))\n",
    "plt.rc('font', size=40)          # controls default text sizes    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=40)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=40) \n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 2, gridspec_kw={'wspace':1/16, 'hspace':1/16}, figsize=(80, 80))\n",
    "\n",
    "sns.boxplot(df['year'],ax=ax1[0])\n",
    "ax1[0].set_title('Years of Release')\n",
    "ax1[0].set_ylabel('Year')\n",
    "\n",
    "sns.boxplot(df['runtime'],ax=ax1[1])\n",
    "ax1[1].set_title('Movie Runtimes')\n",
    "ax1[1].set_ylabel('Runtime')\n",
    "\n",
    "sns.boxplot(df['rating'],ax=ax2[0])\n",
    "ax2[0].set_title('Movie Rating Out of 10')\n",
    "ax2[0].set_ylabel('Rating')\n",
    "\n",
    "sns.boxplot(df['votes'],ax=ax2[1])\n",
    "ax2[1].set_title('Number of Votes for Movie')\n",
    "ax2[1].set_ylabel('Number of Votes')\n",
    "\n",
    "sns.boxplot(df['gross(in $)'],ax=ax3[0])\n",
    "ax3[0].set_title('Gross Income (Millions of $)')\n",
    "ax3[0].set_ylabel('Gross Income (Millions of $)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(ls):\n",
    "    q3= ls.quantile(0.75)\n",
    "    q1= ls.quantile(0.25)\n",
    "    IQR=q3-q1\n",
    "    outliers_upper = ls.loc[ls > q3 + 1.5 * IQR]\n",
    "    outliers_lower = df['runtime'].loc[df['runtime'] < q1 - 1.5 * IQR]\n",
    "    num_outliers_runtime = len(outliers_lower)+len(outliers_upper)\n",
    "    return num_outliers_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of outliers detected in year data: \" + str(outliers(df['year'])))\n",
    "print(\"Number of outliers detected in runtime data: \" + str(outliers(df['runtime'])))\n",
    "print(\"Number of outliers detected in rating data: \" + str(outliers(df['rating'])))\n",
    "print(\"Number of outliers detected in votes data: \" + str(outliers(df['votes'])))\n",
    "print(\"Number of outliers detected in gross income data: \" + str(outliers(df['gross(in $)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Correlation Matrix, dropping Film-Noir since none are in our dataframe anymore from the last 25 years\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We are going to use a few algorithms to approach the solution to this problem, but they are subsets of linear and nonlinear regression. This can be achieved through the use of sklearn's linear_model.LinearRegression (and its relatives Ridge and LASSO regression). In addition to this, we will utilize a decision tree model as well and utilize the machine learning model that scores better on our performance metrics. Our problem is clearly geared towards a regression solution as we aim to predict gross earnings from our features, instead of a classification problem in which logistic regression would be preferred. To test our solution, we will run cross-validation on our dataset to generate an estimate of our model's performance. A possible benchmark is simply guessing the mean gross box office earnings.\n",
    "\n",
    "\n",
    "Because our problem has a very simple goal to achieve, we want to introduce nuance via complex approaches to analyzing the algorithms we use. We’ll use a few different standard model approaches including Gradient Boosting and Random Forest, as well as more complex approaches such as XGBoost and compare the results across each model. Again, because of the simplicity of our problem, we can also extend our analysis to using some hyperparameter optimization techniques, but since models like XGBoost get exceedingly complicated in the number of hyperparameters, we will likely choose instead to do random search and hope to establish good baselines for each approach, especially since we are most likely to use stratified k-fold cross-validation in order to preserve (for example) genre ratio while having a reasonably sized test set (given that our dataset has over 18000 examples). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "To predict the box office gross income of movies based on their year, certificate, runtime, genre, rating and total number of votes, we will be potentially using the following metrics. \n",
    "\n",
    "Mean Absolute Error to measure the average absolute difference between the predicted box office gross and the actual box office gross across all the movies.\n",
    "\n",
    "MAE = (1 / n) * sum(abs(y_true - y_pred))\n",
    "\n",
    "Mean Squared Error and Root Mean Squared Error to measure the square root of the average squared difference between the predicted box office gross and the actual box office gross.\n",
    "\n",
    "MSE = (1 / n) * sum((y_true - y_pred)^2)\n",
    "\n",
    "RMSE = sqrt((1 / n) * sum((y_true - y_pred)^2))\n",
    "\n",
    "And R2 Score to measure how well the regression model fits the actual data\n",
    "\n",
    "R2 Score = 1 - (sum((y_true - y_pred)^2) / sum((y_true - mean(y_true))^2))\n",
    "\n",
    "y_true = actual box office gross\n",
    "\n",
    "y_pred = predicted box office gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[f for f in features if f not in ['gross(in $)']]]\n",
    "                                                    , df['gross(in $)'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train['rating'], bins=20)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Rating')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(X_test['rating'], bins=20)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Rating')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_scaler = StandardScaler()\n",
    "ohe_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[[f for f in features if f not in ['gross(in $)', 'certificate']]]\n",
    "X_train_cat = X_train[['certificate']]\n",
    "\n",
    "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
    "X_train_cat_encoded = ohe_encoder.fit_transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_train_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]),\n",
    "    pd.DataFrame(X_train_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test[[f for f in features if f not in ['gross(in $)', 'certificate']]]\n",
    "X_test_cat = X_test[['certificate']]\n",
    "\n",
    "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
    "X_test_cat_encoded = ohe_encoder.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]),\n",
    "    pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Linear Regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['gross(in $)', 'certificate']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Gross Box Office Earnings\")\n",
    "plt.ylabel(\"Predicted Gross Box Office Earnings\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Decision Tree</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\">Using Year as Categorical (5-year increment)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_category'] = (((df['year'] - 1910) // 5) * 5 + 1910) // 10 * 10\n",
    "df['year_category'] = df['year_category'].apply(lambda x: str(x) + '-' + str(x+4) if (x+3) < 2023 else str(x) + '-2022')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = df['year_category'].value_counts().sort_index()\n",
    "year_counts.plot.bar()\n",
    "plt.title('Distribution of Year Category')\n",
    "plt.xlabel('Year Category')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append('year_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[[f for f in features if f not in ['year', 'gross(in $)']]]\n",
    "                                                    , df['gross(in $)'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train['rating'], bins=20)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Rating')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(X_test['rating'], bins=20)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Rating')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Genres\n",
    "# Extract the genre columns from the train and test sets\n",
    "genre_cols = ['act', 'adv', 'cri', 'fam', 'fan', 'his', 'hor', 'mys', 'sci', 'spo', 'thr', 'war']\n",
    "X_train_genre = X_train[genre_cols]\n",
    "X_test_genre = X_test[genre_cols]\n",
    "\n",
    "# Plot the histograms\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].hist(X_train_genre.values, bins=20, label=genre_cols)\n",
    "ax[0].set_title('Training Set')\n",
    "ax[0].set_xlabel('Genre')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "ax[1].hist(X_test_genre.values, bins=20, label=genre_cols)\n",
    "ax[1].set_title('Test Set')\n",
    "ax[1].set_xlabel('Genre')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]]\n",
    "X_train_cat = X_train[['certificate', 'year_category']]\n",
    "\n",
    "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
    "X_train_cat_encoded = ohe_encoder.fit_transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_train_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "    pd.DataFrame(X_train_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate','year_category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]]\n",
    "X_test_cat = X_test[['certificate', 'year_category',]]\n",
    "\n",
    "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
    "X_test_cat_encoded = ohe_encoder.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.concat([\n",
    "    pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "    pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))\n",
    "], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Gross Box Office Earnings\")\n",
    "plt.ylabel(\"Predicted Gross Box Office Earnings\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=5)\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred= model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]),\n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "feature_names = X_train_preprocessed.columns\n",
    "plt.figure(figsize=(50, 10))\n",
    "plot_tree(model, filled=True, feature_names=feature_names)\n",
    "plt.show()\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel='rbf', C=100, gamma='auto')\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "y_pred = model.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('Mean absolute error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = rf.predict(pd.concat([pd.DataFrame(X_test_num_scaled, columns=[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]), \n",
    "                                  pd.DataFrame(X_test_cat_encoded.toarray(), columns=ohe_encoder.get_feature_names(['certificate', 'year_category']))], axis=1))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization on Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 1000]\n",
    "grid['learning_rate'] = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 5, 7]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=grid, cv=10)\n",
    "# search over our floating values\n",
    "grid_result = grid_search.fit(X=df[[f for f in features if f not in ['year', 'certificate', 'gross(in $)', 'year_category']]], y=df['gross(in $)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMAL: 0.460124 with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "-1.908803 (3.119256) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.908782 (3.118924) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.908568 (3.118712) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.748135 (2.908756) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.747606 (2.907288) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.748217 (2.907749) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.562902 (2.666039) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.563336 (2.666657) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.566180 (2.667023) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "-0.031009 (0.640922) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "-0.038492 (0.641626) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "-0.052397 (0.649654) with params: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.905014 (3.115358) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.905419 (3.115248) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.905277 (3.114987) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.730182 (2.891633) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.731025 (2.892006) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.731429 (2.890582) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.529933 (2.636104) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.530504 (2.635610) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.532874 (2.634360) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.091174 (0.563240) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.085429 (0.563924) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.069836 (0.562684) with params: {'learning_rate': 0.001, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.903728 (3.114629) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.903973 (3.114374) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.903647 (3.114254) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.724582 (2.889216) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.724739 (2.888161) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.724244 (2.886642) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-1.518624 (2.629487) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-1.520363 (2.629334) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-1.519621 (2.626683) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.119153 (0.558221) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.115268 (0.551832) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.100270 (0.535648) with params: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.747563 (2.906170) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.748803 (2.908487) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.747859 (2.907156) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.099727 (2.055599) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.105002 (2.063354) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.106225 (2.060612) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.572994 (1.357851) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.574847 (1.355510) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.578991 (1.356249) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.412467 (0.136139) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.409790 (0.138082) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.396795 (0.157718) with params: {'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.731364 (2.892303) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.731108 (2.891539) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.730804 (2.890185) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.030992 (1.999241) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.034915 (1.999195) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.040571 (1.999732) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.469820 (1.278702) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.472434 (1.276782) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.479878 (1.273815) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.460124 (0.135768) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.453631 (0.138007) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.444410 (0.142801) with params: {'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.723341 (2.888983) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.724306 (2.887481) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.723931 (2.886057) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-1.012353 (1.991259) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-1.010469 (1.988236) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-1.014156 (1.981385) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.440792 (1.266610) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.442537 (1.263222) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.444722 (1.248190) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.459345 (0.138288) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.451154 (0.137297) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.429277 (0.127082) with params: {'learning_rate': 0.005, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.562126 (2.665641) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.563008 (2.664362) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.564560 (2.664656) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.568387 (1.353272) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.575784 (1.355771) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.576555 (1.352257) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "-0.028490 (0.635359) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "-0.035887 (0.638121) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "-0.049653 (0.646205) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.418119 (0.136650) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.413139 (0.137848) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.406975 (0.162318) with params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.525265 (2.632815) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.529679 (2.633274) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.530798 (2.632261) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.463978 (1.274888) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.470490 (1.276479) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.477773 (1.270790) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.092736 (0.558645) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.089262 (0.561926) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.072641 (0.558338) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.448705 (0.130804) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.454709 (0.133785) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.439519 (0.142345) with params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-1.516044 (2.623667) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-1.518678 (2.626706) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-1.517913 (2.624335) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "-0.437745 (1.263656) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "-0.439308 (1.259443) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "-0.441953 (1.245087) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.112998 (0.553198) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.118710 (0.545230) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.103690 (0.531444) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.443402 (0.136197) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.442031 (0.129646) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.420387 (0.130727) with params: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.549305 (1.319546) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.554560 (1.330685) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.561888 (1.335755) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.359359 (0.184497) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.345956 (0.188203) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.338836 (0.183842) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.405546 (0.137952) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.408407 (0.144753) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.396958 (0.153047) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.373746 (0.163250) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.386409 (0.148214) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.373729 (0.162081) with params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.460305 (1.259224) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.454846 (1.254751) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.462102 (1.248957) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.432995 (0.165981) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.429570 (0.163408) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.410608 (0.167196) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.456762 (0.132855) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.444041 (0.133213) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.433070 (0.142340) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.365787 (0.159524) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.396563 (0.161736) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.368527 (0.177576) with params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.434952 (1.235584) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.422734 (1.237843) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.424646 (1.221281) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.433613 (0.164600) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.434108 (0.159325) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.409799 (0.147443) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.451008 (0.141226) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.443895 (0.146431) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.429667 (0.129775) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.368352 (0.173595) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.387612 (0.181521) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.353125 (0.187560) with params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "-0.001864 (0.600995) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "-0.013682 (0.596613) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "-0.016754 (0.602568) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.396932 (0.152632) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.409385 (0.142424) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.398373 (0.155815) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.395229 (0.134968) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.390236 (0.137482) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.407374 (0.158092) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.341013 (0.167623) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.336146 (0.163852) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.325380 (0.184672) with params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.110921 (0.532549) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.116321 (0.517377) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.102272 (0.522668) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.415569 (0.123479) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.441945 (0.141912) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.426083 (0.137655) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.437255 (0.143914) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.440981 (0.147393) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.425569 (0.143877) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.303702 (0.185871) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.349250 (0.171634) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.328390 (0.220500) with params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.136660 (0.510536) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.130773 (0.521542) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.125991 (0.491324) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.424097 (0.150552) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.424123 (0.131187) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.420777 (0.127174) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.405819 (0.133978) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.430700 (0.131868) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.397253 (0.136124) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.324418 (0.198770) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.354576 (0.167942) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.341779 (0.184882) with params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print('OPTIMAL: %f with params: %s' % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "test_means = grid_result.cv_results_['mean_test_score']\n",
    "test_stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for test_mean, test_std, param in zip(test_means, test_stds, params):\n",
    "    print(\"%f (%f) with params: %r\" % (test_mean, test_std, param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "The IMDB dataset is downloaded from the Kaggle website with no personal nor sensitive information included from human participants. All the variables in the dataset are available to public on IMDB website in which we can eliminate the concern for data privacy and misuse of the data. The movie ratings and box office gross are largely influenced by the viewers and fans of particular franchises and movie stars which may be biased, however, since we are using a large dataset with a huge number of ratings, the bias could be neglected or will be evaluated and addressed if the concerns were to arise. Our final product will be available to students of current COGS118A class, future prospective students and potentially to public and we will carefully monitor and identify the unintended use of our project.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"doshinote\"></a>1.[^](#doshi): Doshi, Lyric, et al. Predicting Movie Prices through Dynamic Social Network Analysis - Core. https://core.ac.uk/download/pdf/82778929.pdf. <br> \n",
    "\n",
    "<a name=\"dhirnote\"></a>2.[^](#dhir): Dhir R, Raj A (2018) Movie success prediction using machine learning algorithms and their comparison. In: 2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC), IEEE, pp 385–390. <br>\n",
    "\n",
    "<a name='abidinote'></a>3.[^](#abidi): Abidi, S.M.R., Xu, Y., Ni, J. et al. Popularity prediction of movies: from statistical modeling to machine learning techniques. Multimed Tools Appl 79, 35583–35617 (2020). https://doi.org/10.1007/s11042-019-08546-5. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
